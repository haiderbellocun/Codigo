{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9842c787",
   "metadata": {},
   "source": [
    "## productos ofrecidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabff212",
   "metadata": {},
   "source": [
    "## Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "320ad7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Origen</th>\n",
       "      <th>Usuario_Publico</th>\n",
       "      <th>Post_Date</th>\n",
       "      <th>URL_post</th>\n",
       "      <th>Post</th>\n",
       "      <th>Usu_Comentario</th>\n",
       "      <th>textoComentario</th>\n",
       "      <th>Fecha del Comentario</th>\n",
       "      <th>Red_social</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>areandina</td>\n",
       "      <td>areandina</td>\n",
       "      <td>01/12/2025</td>\n",
       "      <td>https://www.instagram.com/areandina/reel/DRugN...</td>\n",
       "      <td>Tu pr√≥xima alegr√≠a puede ser empezar tu carrer...</td>\n",
       "      <td>katheanz_11</td>\n",
       "      <td>He escrito por varios medios y no me han dado ...</td>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>areandina</td>\n",
       "      <td>areandina</td>\n",
       "      <td>30/12/2025</td>\n",
       "      <td>https://www.instagram.com/areandina/reel/DS5j4...</td>\n",
       "      <td>El 2025 nos dej√≥ momentos que se quedan en el ...</td>\n",
       "      <td>orozco872zdzocco</td>\n",
       "      <td>y seguiremos creciendo &amp; haciendo marcar sue√±o...</td>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>areandina</td>\n",
       "      <td>areandina</td>\n",
       "      <td>16/12/2025</td>\n",
       "      <td>https://www.instagram.com/areandina/reel/DSVgQ...</td>\n",
       "      <td>Sus historias comenzaron en distintos lugares ...</td>\n",
       "      <td>anamaria_pineda</td>\n",
       "      <td>Yo fui una de las graduandos hoy,,, y todo lo ...</td>\n",
       "      <td>2025-12-23</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>areandina</td>\n",
       "      <td>areandina</td>\n",
       "      <td>16/12/2025</td>\n",
       "      <td>https://www.instagram.com/areandina/reel/DSVgQ...</td>\n",
       "      <td>Sus historias comenzaron en distintos lugares ...</td>\n",
       "      <td>alejaa.diaz0514</td>\n",
       "      <td>Me gradu√© en Administraci√≥n de empresas el 16 ...</td>\n",
       "      <td>2025-12-23</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>areandina</td>\n",
       "      <td>areandina</td>\n",
       "      <td>25/11/2025</td>\n",
       "      <td>https://www.instagram.com/areandina/reel/DRfep...</td>\n",
       "      <td>¬øTu vibra? Pensar en grande.\\n¬øTu camino? Nego...</td>\n",
       "      <td>elkinc.lara_7</td>\n",
       "      <td>üëèüî•\\n1 Me gusta\\nResponder</td>\n",
       "      <td>2025-12-02</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>yosoycun</td>\n",
       "      <td>yosoycun</td>\n",
       "      <td>01/01/2026</td>\n",
       "      <td>https://www.instagram.com/yeralce_/reel/DS-qky...</td>\n",
       "      <td>No es otro a√±o.\\nEs el a√±o donde te la crees.\\...</td>\n",
       "      <td>yeralce_</td>\n",
       "      <td>No es otro a√±o.\\nEs el a√±o donde te la crees.\\...</td>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>yosoycun</td>\n",
       "      <td>yosoycun</td>\n",
       "      <td>01/01/2026</td>\n",
       "      <td>https://www.instagram.com/yeralce_/reel/DS-qky...</td>\n",
       "      <td>No es otro a√±o.\\nEs el a√±o donde te la crees.\\...</td>\n",
       "      <td>johana0895</td>\n",
       "      <td>Para cu√°ndo las fechas de grados 2026?\\n1 Me g...</td>\n",
       "      <td>2026-01-02</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>yosoycun</td>\n",
       "      <td>yosoycun</td>\n",
       "      <td>01/01/2026</td>\n",
       "      <td>https://www.instagram.com/yeralce_/reel/DS-qky...</td>\n",
       "      <td>No es otro a√±o.\\nEs el a√±o donde te la crees.\\...</td>\n",
       "      <td>nata_manchola</td>\n",
       "      <td>Ya est√°n publicadas grados cun\\n1 Me gusta\\nRe...</td>\n",
       "      <td>2026-01-06</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2838</th>\n",
       "      <td>yosoycun</td>\n",
       "      <td>yosoycun</td>\n",
       "      <td>01/01/2026</td>\n",
       "      <td>https://www.instagram.com/jeis0n0_0/reel/DS-cn...</td>\n",
       "      <td>1 de enero de 2026.\\nNuevo a√±o, nuevas oportun...</td>\n",
       "      <td>jeis0n0_0</td>\n",
       "      <td>1 de enero de 2026.\\nNuevo a√±o, nuevas oportun...</td>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839</th>\n",
       "      <td>yosoycun</td>\n",
       "      <td>yosoycun</td>\n",
       "      <td>01/01/2026</td>\n",
       "      <td>https://www.instagram.com/jeis0n0_0/reel/DS-cn...</td>\n",
       "      <td>1 de enero de 2026.\\nNuevo a√±o, nuevas oportun...</td>\n",
       "      <td>proyectos_especiales_cun</td>\n",
       "      <td>üëèüëèüëèüî•üî•üî•\\nResponder</td>\n",
       "      <td>2026-01-03</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2840 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Origen  Usuario_Publico   Post_Date  \\\n",
       "0     areandina       areandina  01/12/2025   \n",
       "1     areandina       areandina  30/12/2025   \n",
       "2     areandina       areandina  16/12/2025   \n",
       "3     areandina       areandina  16/12/2025   \n",
       "4     areandina       areandina  25/11/2025   \n",
       "...         ...             ...         ...   \n",
       "2835   yosoycun        yosoycun  01/01/2026   \n",
       "2836   yosoycun        yosoycun  01/01/2026   \n",
       "2837   yosoycun        yosoycun  01/01/2026   \n",
       "2838   yosoycun        yosoycun  01/01/2026   \n",
       "2839   yosoycun        yosoycun  01/01/2026   \n",
       "\n",
       "                                               URL_post  \\\n",
       "0     https://www.instagram.com/areandina/reel/DRugN...   \n",
       "1     https://www.instagram.com/areandina/reel/DS5j4...   \n",
       "2     https://www.instagram.com/areandina/reel/DSVgQ...   \n",
       "3     https://www.instagram.com/areandina/reel/DSVgQ...   \n",
       "4     https://www.instagram.com/areandina/reel/DRfep...   \n",
       "...                                                 ...   \n",
       "2835  https://www.instagram.com/yeralce_/reel/DS-qky...   \n",
       "2836  https://www.instagram.com/yeralce_/reel/DS-qky...   \n",
       "2837  https://www.instagram.com/yeralce_/reel/DS-qky...   \n",
       "2838  https://www.instagram.com/jeis0n0_0/reel/DS-cn...   \n",
       "2839  https://www.instagram.com/jeis0n0_0/reel/DS-cn...   \n",
       "\n",
       "                                                   Post  \\\n",
       "0     Tu pr√≥xima alegr√≠a puede ser empezar tu carrer...   \n",
       "1     El 2025 nos dej√≥ momentos que se quedan en el ...   \n",
       "2     Sus historias comenzaron en distintos lugares ...   \n",
       "3     Sus historias comenzaron en distintos lugares ...   \n",
       "4     ¬øTu vibra? Pensar en grande.\\n¬øTu camino? Nego...   \n",
       "...                                                 ...   \n",
       "2835  No es otro a√±o.\\nEs el a√±o donde te la crees.\\...   \n",
       "2836  No es otro a√±o.\\nEs el a√±o donde te la crees.\\...   \n",
       "2837  No es otro a√±o.\\nEs el a√±o donde te la crees.\\...   \n",
       "2838  1 de enero de 2026.\\nNuevo a√±o, nuevas oportun...   \n",
       "2839  1 de enero de 2026.\\nNuevo a√±o, nuevas oportun...   \n",
       "\n",
       "                Usu_Comentario  \\\n",
       "0                  katheanz_11   \n",
       "1             orozco872zdzocco   \n",
       "2              anamaria_pineda   \n",
       "3              alejaa.diaz0514   \n",
       "4                elkinc.lara_7   \n",
       "...                        ...   \n",
       "2835                  yeralce_   \n",
       "2836                johana0895   \n",
       "2837             nata_manchola   \n",
       "2838                 jeis0n0_0   \n",
       "2839  proyectos_especiales_cun   \n",
       "\n",
       "                                        textoComentario Fecha del Comentario  \\\n",
       "0     He escrito por varios medios y no me han dado ...           2025-12-30   \n",
       "1     y seguiremos creciendo & haciendo marcar sue√±o...           2025-12-31   \n",
       "2     Yo fui una de las graduandos hoy,,, y todo lo ...           2025-12-23   \n",
       "3     Me gradu√© en Administraci√≥n de empresas el 16 ...           2025-12-23   \n",
       "4                             üëèüî•\\n1 Me gusta\\nResponder           2025-12-02   \n",
       "...                                                 ...                  ...   \n",
       "2835  No es otro a√±o.\\nEs el a√±o donde te la crees.\\...           2026-01-01   \n",
       "2836  Para cu√°ndo las fechas de grados 2026?\\n1 Me g...           2026-01-02   \n",
       "2837  Ya est√°n publicadas grados cun\\n1 Me gusta\\nRe...           2026-01-06   \n",
       "2838  1 de enero de 2026.\\nNuevo a√±o, nuevas oportun...           2026-01-01   \n",
       "2839                                  üëèüëèüëèüî•üî•üî•\\nResponder           2026-01-03   \n",
       "\n",
       "     Red_social  \n",
       "0     instagram  \n",
       "1     instagram  \n",
       "2     instagram  \n",
       "3     instagram  \n",
       "4     instagram  \n",
       "...         ...  \n",
       "2835  instagram  \n",
       "2836  instagram  \n",
       "2837  instagram  \n",
       "2838  instagram  \n",
       "2839  instagram  \n",
       "\n",
       "[2840 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(r\"C:\\Users\\haider_bello\\Downloads\\estilo-de-gradiente-del-esquema-de-la-base-de-datos\\consolidado_fecha_normal.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2297e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama activo en http://localhost:11434\n",
      "üìÇ Dataset cargado: (2840, 9)\n",
      "üöÄ Iniciando Comentarios (2840 docs)...\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# === ESCUCHA SOCIAL EXPERTA (Ollama) ‚Üí df_final + df_sql listo para SQL ===\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from string import Template\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import Counter\n",
    "\n",
    "# ---------------------- CONFIGURACI√ìN ----------------------\n",
    "OLLAMA_URL    = \"http://localhost:11434\"\n",
    "MODEL         = \"qwen2.5:7b-instruct\"\n",
    "MAX_WORKERS   = 3\n",
    "PRINT_EVERY   = 50\n",
    "TIMEOUT       = 180\n",
    "RETRIES       = 2\n",
    "\n",
    "LIMIT         = None  # None = todo\n",
    "\n",
    "# ======= ENTRADA (en notebook/VS Code df debe existir) =======\n",
    "INPUT_DF = df  # asume que df ya est√° cargado en memoria\n",
    "\n",
    "# Columnas de tu dataset (origen)\n",
    "COL_POST_IN     = \"Post\"\n",
    "COL_COMENT_IN   = \"textoComentario\"       # si tu DF ya trae \"Comentario\", se usa ese\n",
    "COL_FECHA_IN    = \"Fecha del comentario\"  # debe coincidir con tu DF\n",
    "\n",
    "EXPORT_XLSX = \"analisis_marketing_productos_insta_6_enero_FINAL.xlsx\"\n",
    "\n",
    "\n",
    "def check_ollama():\n",
    "    r = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=5)\n",
    "    r.raise_for_status()\n",
    "    print(f\"‚úÖ Ollama activo en {OLLAMA_URL}\")\n",
    "\n",
    "\n",
    "# ---- LISTAS Y REGLAS (Negocio) ----\n",
    "ALLOWED_SENT = {\n",
    "    \"gratitud_alegria\",\"confianza_seguridad\",\"sorpresa\",\n",
    "    \"tristeza\",\"miedo_preocupacion\",\"desagrado\",\"enojo_frustracion\",\n",
    "    \"consulta_sugerencia\"\n",
    "}\n",
    "\n",
    "FEAR_EMOJI = {\"üòü\",\"üò∞\",\"üò®\",\"üò±\",\"üò•\",\"üòì\",\"üòî\"}\n",
    "fear_regex = re.compile(r\"\\b(miedo|temo|temer|preocupad\\w*|ansied\\w*|p[a√°]nico|asustad\\w*)\\b\", re.IGNORECASE)\n",
    "neg_strong = re.compile(r\"\\b(p[e√©]simo|estafa|robo|asco|in[u√∫]tiles|ladrones|enga√±)\\b\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def map_tipo_from_sent(sent):\n",
    "    if sent in {\"gratitud_alegria\",\"confianza_seguridad\",\"sorpresa\"}:\n",
    "        return \"felicitacion_positivo\"\n",
    "    if sent in {\"tristeza\",\"miedo_preocupacion\",\"desagrado\",\"enojo_frustracion\"}:\n",
    "        return \"queja_reclamo_negativo\"\n",
    "    if sent == \"consulta_sugerencia\":\n",
    "        return \"pregunta_neutral\"\n",
    "    return \"otro\"\n",
    "\n",
    "\n",
    "def fix_miedo(texto: str, sent: str) -> str:\n",
    "    if sent != \"miedo_preocupacion\":\n",
    "        return sent\n",
    "    t = (texto or \"\").lower()\n",
    "    has_signal = any(e in t for e in FEAR_EMOJI) or bool(fear_regex.search(t))\n",
    "    if not has_signal:\n",
    "        if neg_strong.search(t):\n",
    "            return \"enojo_frustracion\"\n",
    "        return \"consulta_sugerencia\"\n",
    "    return sent\n",
    "\n",
    "\n",
    "def extract_json(s: str):\n",
    "    \"\"\"Extractor robusto de JSON desde la respuesta del LLM.\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    start, end = s.find(\"{\"), s.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        chunk = s[start:end+1]\n",
    "        try:\n",
    "            return json.loads(chunk)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    m = re.search(r\"\\{.*\\}\", s, re.DOTALL)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def infer_tema_fallback(texto: str) -> str:\n",
    "    t = (texto or \"\").lower()\n",
    "    if \"inscrip\" in t or \"matr\" in t:\n",
    "        return \"admisiones_matricula\"\n",
    "    if \"pag\" in t or \"cost\" in t:\n",
    "        return \"pagos_finanzas\"\n",
    "    if \"clase\" in t or \"curso\" in t:\n",
    "        return \"educacion_academico\"\n",
    "    return \"comunidad\"\n",
    "\n",
    "\n",
    "# ===== PROMPTS =====\n",
    "PROMPT_COMENT = Template(\"\"\"\n",
    "Eres analista de Customer Experience. Clasifica este COMENTARIO:\n",
    "1. SENTIMIENTO: gratitud_alegria | confianza_seguridad | sorpresa | tristeza | miedo_preocupacion | desagrado | enojo_frustracion | consulta_sugerencia\n",
    "2. TIPO: felicitacion_positivo | queja_reclamo_negativo | pregunta_neutral\n",
    "3. TEMA: educacion_academico | servicio_atencion | pagos_finanzas | admisiones_matricula | eventos | infraestructura | comunidad | empleo_practicas | comunicaciones_marketing | otro\n",
    "4. CLASE: elogio | queja | pregunta | sugerencia | experiencia | spam_bot | offtopic | otro\n",
    "Responde SOLO JSON: {\"sentimiento\":\"...\", \"tipo_comentario\":\"...\", \"tema\":\"...\", \"clase_comentario\":\"...\", \"justificacion\":\"...\"}\n",
    "Texto: \\\"\\\"\\\"$texto\\\"\\\"\\\" \n",
    "\"\"\")\n",
    "\n",
    "PROMPT_POST = Template(\"\"\"\n",
    "Eres experto en Marketing. Analiza el POST y extrae la oferta comercial:\n",
    "1. TEMA_POST: (educacion_academico, servicio_atencion, pagos_finanzas, admisiones_matricula, eventos, infraestructura, comunidad, empleo_practicas, comunicaciones_marketing, otro)\n",
    "2. CLASE_POST: (informativo, promocional, convocatoria, evento, logro_testimonial, entretenimiento, servicio_atencion, comunidad, otro)\n",
    "3. PRODUCTO_DETECTADO:\n",
    "   - Nombre EXACTO del producto/beneficio/programa ofrecido.\n",
    "   - Si no hay oferta clara, pon \"ninguno\".\n",
    "   - M√°ximo 5 palabras.\n",
    "Responde SOLO JSON: {\"tema_post\":\"...\", \"clase_post\":\"...\", \"producto_detectado\":\"...\", \"justificacion_post\":\"...\"}\n",
    "Texto: \\\"\\\"\\\"$texto\\\"\\\"\\\" \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def classify_comment(texto, retries=RETRIES):\n",
    "    if not str(texto).strip():\n",
    "        return {\n",
    "            \"sentimiento\": \"consulta_sugerencia\",\n",
    "            \"tipo_comentario\": \"pregunta_neutral\",\n",
    "            \"tema\": \"comunidad\",\n",
    "            \"clase_comentario\": \"otro\",\n",
    "            \"justificacion\": \"vacio\"\n",
    "        }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": PROMPT_COMENT.substitute(texto=texto),\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0}\n",
    "    }\n",
    "\n",
    "    for _ in range(retries + 1):\n",
    "        try:\n",
    "            r = requests.post(f\"{OLLAMA_URL}/api/generate\", json=payload, timeout=TIMEOUT)\n",
    "            r.raise_for_status()\n",
    "            j = extract_json(r.json().get(\"response\", \"\")) or {}\n",
    "\n",
    "            sent = fix_miedo(texto, (j.get(\"sentimiento\") or \"\").lower())\n",
    "            if sent not in ALLOWED_SENT:\n",
    "                sent = \"consulta_sugerencia\"\n",
    "\n",
    "            tema = (j.get(\"tema\") or \"\").lower()\n",
    "            if tema == \"otro\":\n",
    "                tema = infer_tema_fallback(texto)\n",
    "            if tema == \"otro\":\n",
    "                tema = \"comunidad\"\n",
    "\n",
    "            tipo = j.get(\"tipo_comentario\") or map_tipo_from_sent(sent)\n",
    "\n",
    "            return {\n",
    "                \"sentimiento\": sent,\n",
    "                \"tipo_comentario\": tipo,\n",
    "                \"tema\": tema,\n",
    "                \"clase_comentario\": j.get(\"clase_comentario\", \"otro\"),\n",
    "                \"justificacion\": j.get(\"justificacion\", \"\")\n",
    "            }\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return {\n",
    "        \"sentimiento\": \"consulta_sugerencia\",\n",
    "        \"tipo_comentario\": \"pregunta_neutral\",\n",
    "        \"tema\": \"comunidad\",\n",
    "        \"clase_comentario\": \"otro\",\n",
    "        \"justificacion\": \"error_ollama\"\n",
    "    }\n",
    "\n",
    "\n",
    "def classify_post(texto, retries=RETRIES):\n",
    "    if not str(texto).strip():\n",
    "        return {\n",
    "            \"tema_post\": \"comunidad\",\n",
    "            \"clase_post\": \"otro\",\n",
    "            \"producto_detectado\": \"ninguno\",\n",
    "            \"justificacion_post\": \"vacio\"\n",
    "        }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": PROMPT_POST.substitute(texto=texto),\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0.1}\n",
    "    }\n",
    "\n",
    "    for _ in range(retries + 1):\n",
    "        try:\n",
    "            r = requests.post(f\"{OLLAMA_URL}/api/generate\", json=payload, timeout=TIMEOUT)\n",
    "            r.raise_for_status()\n",
    "            j = extract_json(r.json().get(\"response\", \"\")) or {}\n",
    "\n",
    "            tema = (j.get(\"tema_post\") or \"\").lower()\n",
    "            if tema == \"otro\":\n",
    "                tema = infer_tema_fallback(texto)\n",
    "            if tema == \"otro\":\n",
    "                tema = \"comunidad\"\n",
    "\n",
    "            prod = str(j.get(\"producto_detectado\", \"ninguno\")).strip().strip('.\"').lower()\n",
    "            if prod in {\"ninguno\",\"n/a\",\"no aplica\",\"no\",\"informacion\"}:\n",
    "                prod = \"ninguno\"\n",
    "\n",
    "            return {\n",
    "                \"tema_post\": tema,\n",
    "                \"clase_post\": j.get(\"clase_post\", \"otro\"),\n",
    "                \"producto_detectado\": prod,\n",
    "                \"justificacion_post\": j.get(\"justificacion_post\", \"\")\n",
    "            }\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return {\n",
    "        \"tema_post\": \"comunidad\",\n",
    "        \"clase_post\": \"otro\",\n",
    "        \"producto_detectado\": \"error\",\n",
    "        \"justificacion_post\": \"error_ollama\"\n",
    "    }\n",
    "\n",
    "\n",
    "def classify_series_parallel(df_source, colname, fn, rename_map, label):\n",
    "    if colname not in df_source.columns:\n",
    "        print(f\"‚ö†Ô∏è No existe columna '{colname}' para {label}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = df_source[colname].fillna(\"\").astype(str).tolist()\n",
    "    total = len(data)\n",
    "    results = [None] * total\n",
    "\n",
    "    print(f\"üöÄ Iniciando {label} ({total} docs)...\")\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futures = {ex.submit(fn, txt): i for i, txt in enumerate(data)}\n",
    "        completed = 0\n",
    "        for fut in as_completed(futures):\n",
    "            results[futures[fut]] = fut.result()\n",
    "            completed += 1\n",
    "            if completed % PRINT_EVERY == 0:\n",
    "                print(f\"   [{label}] {completed}/{total}...\", end=\"\\r\")\n",
    "\n",
    "    print(f\"‚úÖ {label} terminado.              \")\n",
    "    return pd.DataFrame(results).rename(columns=rename_map)\n",
    "\n",
    "\n",
    "# ---- keywords -> palabras_clave_comentario ----\n",
    "_STOP = {\n",
    "    \"que\",\"de\",\"la\",\"el\",\"en\",\"y\",\"a\",\"los\",\"las\",\"un\",\"una\",\"por\",\"para\",\"con\",\"no\",\"si\",\n",
    "    \"es\",\"al\",\"lo\",\"se\",\"mi\",\"tu\",\"su\",\"me\",\"te\",\"le\",\"les\",\"ya\",\"muy\",\"m√°s\",\"mas\",\"pero\",\n",
    "    \"como\",\"cuando\",\"donde\",\"del\",\"hay\",\"son\",\"uno\",\"este\",\"esta\",\"eso\",\"esa\",\"una\"\n",
    "}\n",
    "\n",
    "def extraer_keywords(texto, topk=5):\n",
    "    t = (texto or \"\").lower()\n",
    "    toks = re.findall(r\"[a-z√°√©√≠√≥√∫√±]{3,}\", t)\n",
    "    toks = [w for w in toks if w not in _STOP]\n",
    "    if not toks:\n",
    "        return None\n",
    "    top = [w for w, _ in Counter(toks).most_common(topk)]\n",
    "    return \", \".join(top)\n",
    "\n",
    "\n",
    "def normalizar_input(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df0 = df_in.copy()\n",
    "    df0 = df0.loc[:, ~df0.columns.duplicated(keep=\"last\")]\n",
    "\n",
    "    # Normaliza Comentario\n",
    "    if \"Comentario\" not in df0.columns and COL_COMENT_IN in df0.columns:\n",
    "        df0 = df0.rename(columns={COL_COMENT_IN: \"Comentario\"})\n",
    "\n",
    "    # Normaliza Post\n",
    "    if \"Post\" not in df0.columns and COL_POST_IN in df0.columns:\n",
    "        df0 = df0.rename(columns={COL_POST_IN: \"Post\"})\n",
    "\n",
    "    return df0\n",
    "\n",
    "\n",
    "def enrich_data(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df0 = df_in.copy()\n",
    "    df0 = df0.loc[:, ~df0.columns.duplicated(keep=\"last\")]\n",
    "\n",
    "    # Fechas => anio, mes, semana, dia_semana\n",
    "    if COL_FECHA_IN in df0.columns:\n",
    "        dt = pd.to_datetime(df0[COL_FECHA_IN], errors=\"coerce\")\n",
    "        df0[\"anio\"] = dt.dt.year.astype(\"Int64\")\n",
    "        df0[\"mes\"] = dt.dt.month.astype(\"Int64\")\n",
    "        df0[\"semana\"] = dt.dt.isocalendar().week.astype(\"Int64\")\n",
    "        df0[\"dia_semana\"] = dt.dt.weekday.astype(\"Int64\")  # 0=Lun ... 6=Dom\n",
    "    else:\n",
    "        for c in [\"anio\",\"mes\",\"semana\",\"dia_semana\"]:\n",
    "            if c not in df0.columns:\n",
    "                df0[c] = pd.NA\n",
    "\n",
    "    # Polaridad y tipo emoci√≥n\n",
    "    pol_map = {\n",
    "        \"gratitud_alegria\": 1, \"confianza_seguridad\": 1, \"sorpresa\": 0.5,\n",
    "        \"consulta_sugerencia\": 0, \"pregunta_neutral\": 0,\n",
    "        \"tristeza\": -0.5, \"miedo_preocupacion\": -1, \"desagrado\": -1, \"enojo_frustracion\": -1\n",
    "    }\n",
    "\n",
    "    if \"sentimiento_comentario\" in df0.columns:\n",
    "        s = df0[\"sentimiento_comentario\"].astype(str).str.lower()\n",
    "        df0[\"polaridad_comentario\"] = s.map(pol_map).fillna(0)\n",
    "\n",
    "        pos = {\"gratitud_alegria\",\"confianza_seguridad\",\"sorpresa\"}\n",
    "        neg = {\"tristeza\",\"miedo_preocupacion\",\"desagrado\",\"enojo_frustracion\"}\n",
    "        df0[\"emocion_tipo_comentario\"] = s.map(\n",
    "            lambda x: \"positiva\" if x in pos else (\"negativa\" if x in neg else \"neutral\")\n",
    "        )\n",
    "    else:\n",
    "        df0[\"polaridad_comentario\"] = 0\n",
    "        df0[\"emocion_tipo_comentario\"] = pd.NA\n",
    "\n",
    "    # Keywords\n",
    "    if \"Comentario\" in df0.columns:\n",
    "        df0[\"palabras_clave_comentario\"] = df0[\"Comentario\"].astype(str).apply(extraer_keywords)\n",
    "    else:\n",
    "        df0[\"palabras_clave_comentario\"] = pd.NA\n",
    "\n",
    "    return df0\n",
    "\n",
    "\n",
    "def get_product_stats(df0: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"producto_oferta\" not in df0.columns:\n",
    "        return pd.DataFrame()\n",
    "    ignore = {\"ninguno\", \"error\", \"n/a\", \"comunidad\", \"informacion\", \"foto\", \"post\", \"nan\", \"none\"}\n",
    "    series = df0[\"producto_oferta\"].astype(str).str.lower().str.strip()\n",
    "    counts = series[~series.isin(ignore)].value_counts().reset_index()\n",
    "    counts.columns = [\"Producto_Detectado\", \"Menciones_en_Posts\"]\n",
    "    return counts.head(60)\n",
    "\n",
    "\n",
    "def preparar_df_sql(df_final: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    df_sql con el schema EXACTO de tu tabla (seg√∫n la captura).\n",
    "    NOTA: la tabla NO incluye 'producto_oferta'. Si la necesitas en SQL, agrega la columna en la tabla.\n",
    "    \"\"\"\n",
    "    df0 = df_final.copy()\n",
    "    df0 = df0.loc[:, ~df0.columns.duplicated(keep=\"last\")]\n",
    "\n",
    "    required_cols = [\n",
    "        \"Post\",\n",
    "        \"Comentario\",\n",
    "        \"Fecha del comentario\",\n",
    "        \"Origen\",\n",
    "        \"Usu_Comentario\",\n",
    "        \"URL_post\",\n",
    "        \"Red_social\",\n",
    "        \"Post_Date\",\n",
    "        \"Traza\",\n",
    "        \"ID_comentario\",\n",
    "        \"sentimiento_comentario\",\n",
    "        \"tipo_comentario_comentario\",\n",
    "        \"tema_comentario\",\n",
    "        \"clase_comentario\",\n",
    "        \"justificacion_comentario\",\n",
    "        \"tema_post\",\n",
    "        \"clase_post\",\n",
    "        \"justificacion_post\",\n",
    "        \"anio\",\n",
    "        \"mes\",\n",
    "        \"semana\",\n",
    "        \"dia_semana\",\n",
    "        \"polaridad_comentario\",\n",
    "        \"emocion_tipo_comentario\",\n",
    "        \"palabras_clave_comentario\",\n",
    "    ]\n",
    "\n",
    "    for c in required_cols:\n",
    "        if c not in df0.columns:\n",
    "            df0[c] = pd.NA\n",
    "\n",
    "    return df0[required_cols]\n",
    "\n",
    "\n",
    "# ------------------ EJECUCI√ìN ------------------\n",
    "check_ollama()\n",
    "\n",
    "df_work = normalizar_input(INPUT_DF)\n",
    "if LIMIT:\n",
    "    df_work = df_work.head(LIMIT)\n",
    "\n",
    "print(f\"üìÇ Dataset cargado: {df_work.shape}\")\n",
    "\n",
    "# 1) Comentarios\n",
    "res_com = classify_series_parallel(\n",
    "    df_work, \"Comentario\", classify_comment,\n",
    "    rename_map={\n",
    "        \"sentimiento\": \"sentimiento_comentario\",\n",
    "        \"tipo_comentario\": \"tipo_comentario_comentario\",  # <-- como tu SQL\n",
    "        \"tema\": \"tema_comentario\",\n",
    "        \"clase_comentario\": \"clase_comentario\",\n",
    "        \"justificacion\": \"justificacion_comentario\",\n",
    "    },\n",
    "    label=\"Comentarios\"\n",
    ")\n",
    "\n",
    "# 2) Posts + producto\n",
    "res_post = classify_series_parallel(\n",
    "    df_work, \"Post\", classify_post,\n",
    "    rename_map={\n",
    "        \"tema_post\": \"tema_post\",\n",
    "        \"clase_post\": \"clase_post\",\n",
    "        \"producto_detectado\": \"producto_oferta\",           # <-- se queda en df_final (no en df_sql)\n",
    "        \"justificacion_post\": \"justificacion_post\",\n",
    "    },\n",
    "    label=\"Posts\"\n",
    ")\n",
    "\n",
    "# 3) Evita duplicados antes del concat\n",
    "cols_nuevas = set(res_com.columns).union(set(res_post.columns))\n",
    "df_work_clean = df_work.drop(columns=[c for c in cols_nuevas if c in df_work.columns], errors=\"ignore\")\n",
    "\n",
    "# 4) df_final\n",
    "df_final = pd.concat([df_work_clean.reset_index(drop=True), res_com, res_post], axis=1)\n",
    "\n",
    "# 5) Enriquecimiento => columnas del schema (anio, mes, semana, dia_semana, polaridad, emoci√≥n, keywords)\n",
    "df_final = enrich_data(df_final)\n",
    "\n",
    "# 6) Ranking productos (opcional)\n",
    "stats_prod = get_product_stats(df_final)\n",
    "\n",
    "# 7) df_sql listo para insertar (schema igual al de tu tabla)\n",
    "df_sql = preparar_df_sql(df_final)\n",
    "\n",
    "# --- sanity check ---\n",
    "print(\"‚úÖ df_final cols:\", len(df_final.columns))\n",
    "print(\"‚úÖ df_sql cols:\", df_sql.columns.tolist())\n",
    "print(df_sql.head(2))\n",
    "\n",
    "# 8) Export Excel (multihoja)\n",
    "print(f\"üíæ Generando Excel: {EXPORT_XLSX}\")\n",
    "try:\n",
    "    with pd.ExcelWriter(EXPORT_XLSX, engine=\"openpyxl\") as writer:\n",
    "        df_final.to_excel(writer, index=False, sheet_name=\"Data_Clasificada\")\n",
    "        df_sql.to_excel(writer, index=False, sheet_name=\"SQL_READY\")\n",
    "        if not stats_prod.empty:\n",
    "            stats_prod.to_excel(writer, index=False, sheet_name=\"Ranking_Productos\")\n",
    "        if \"tema_comentario\" in df_final.columns:\n",
    "            df_final[\"tema_comentario\"].value_counts().reset_index().to_excel(\n",
    "                writer, index=False, sheet_name=\"Temas\"\n",
    "            )\n",
    "    print(\"\\n‚ú® ¬°Proceso completado! Revisa 'SQL_READY' para cargar a SQL.\")\n",
    "except PermissionError:\n",
    "    print(\"‚ùå ERROR: El archivo Excel est√° abierto. Ci√©rralo e intenta de nuevo.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
